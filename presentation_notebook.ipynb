{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, balanced_accuracy_score\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix/initiate randomness\n",
    "seed = 42\n",
    "rs = np.random.RandomState(np.random.MT19937(np.random.SeedSequence(seed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a (binary) classification data set which has 1000 samples and 100 features. Out of the 100 features\n",
    "* 1 really contains information\n",
    "* `class_sep` which indicates how easy the classification task will be (larger numbers -> easier task) set to 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features, n_classes = 1_000, 100, 2\n",
    "X, y = make_classification(n_samples=n_samples, n_features=n_features, n_classes=n_classes, n_informative=1, n_redundant=0, n_repeated=0, class_sep=0.2, n_clusters_per_class=1, random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([499, 501]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various helper functions \n",
    "\n",
    "def get_models(random_state):\n",
    "    return LogisticRegression(penalty=\"none\"), RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "\n",
    "def report_performance(model, X, y, cv=None):\n",
    "    model_name = model.__class__.__name__\n",
    "    if cv is not None:\n",
    "        AUC_mean = cv.test_roc_auc.mean()\n",
    "        AUC_std = cv.test_roc_auc.std()\n",
    "        ACC_mean = cv.test_balanced_accuracy.mean()\n",
    "        ACC_std = cv.test_balanced_accuracy.std()\n",
    "        perf_string = f\"AUC={AUC_mean:.4f} ({AUC_std:.4f}); ACC={ACC_mean:.4f} ({ACC_std:.4f})\"\n",
    "    else:\n",
    "        AUC = roc_auc_score(y_true=y, y_score=model.predict_proba(X)[:, 1])\n",
    "        ACC = balanced_accuracy_score(y_true=y, y_pred=model.predict(X))\n",
    "        perf_string = f\"AUC={AUC:.4f}; ACC={ACC:.4f}\"\n",
    "\n",
    "    print(f\"{model_name}: \" + perf_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following experiments we will create two models:\n",
    "1. A standard linear Logistic Regression (with no penalty)\n",
    "2. A standard non-linear model: Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model, nonlinear_model = get_models(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Experiment: Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Wrong way: train + test on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train+Test on same data = overfitting:\n",
      "LogisticRegression: AUC=0.7320; ACC=0.6590\n",
      "RandomForestClassifier: AUC=1.0000; ACC=1.0000\n"
     ]
    }
   ],
   "source": [
    "linear_model.fit(X, y)\n",
    "nonlinear_model.fit(X, y)\n",
    "\n",
    "print(\"Train+Test on same data = overfitting:\")\n",
    "report_performance(linear_model, X, y)\n",
    "report_performance(nonlinear_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Right way: train + test using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train+Test via cross-validation = good:\n",
      "LogisticRegression: AUC=0.5982 (0.0491); ACC=0.5659 (0.0430)\n",
      "RandomForestClassifier: AUC=0.6017 (0.0502); ACC=0.5672 (0.0475)\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "cv_scores_lin = cross_validate(clone(linear_model), X, y, cv=cv, scoring=[\"roc_auc\", \"balanced_accuracy\"], n_jobs=2)\n",
    "cv_scores_nlin = cross_validate(clone(nonlinear_model), X, y, cv=cv, scoring=[\"roc_auc\", \"balanced_accuracy\"], n_jobs=2)\n",
    "\n",
    "print(\"Train+Test via cross-validation = good:\")\n",
    "report_performance(linear_model, X, y, cv=pd.DataFrame(cv_scores_lin))\n",
    "report_performance(nonlinear_model, X, y, cv=pd.DataFrame(cv_scores_nlin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Summary*:\n",
    "- Nothing magical here: if you fit and validate your model on the same data set you will overfit\n",
    "- Non-linear models can overfit more easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Experiment: Data Leakage during Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Wrong way: Perform feature selection on the whole data set and then use cross-validation to fit/validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection on all data + cross-validation afterwards = bias:\n",
      "LogisticRegression: AUC=0.6798 (0.0518); ACC=0.6250 (0.0420)\n",
      "RandomForestClassifier: AUC=0.6661 (0.0398); ACC=0.6251 (0.0325)\n"
     ]
    }
   ],
   "source": [
    "feature_selection = SelectPercentile(percentile=10)\n",
    "X_sel = feature_selection.fit_transform(X, y)\n",
    "\n",
    "cv_scores_lin = cross_validate(clone(linear_model), X_sel, y, cv=cv, scoring=[\"roc_auc\", \"balanced_accuracy\"], n_jobs=2)\n",
    "cv_scores_nlin = cross_validate(clone(nonlinear_model), X_sel, y, cv=cv, scoring=[\"roc_auc\", \"balanced_accuracy\"], n_jobs=2)\n",
    "\n",
    "print(\"Feature selection on all data + cross-validation afterwards = bias:\")\n",
    "report_performance(linear_model, X, y, cv=pd.DataFrame(cv_scores_lin))\n",
    "report_performance(nonlinear_model, X, y, cv=pd.DataFrame(cv_scores_nlin))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Right way: Feature selection + model fit only on the training set, validation of the whole pipeline on the independent test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection + modeling via cross-validation = good:\n",
      "Pipeline: AUC=0.6348 (0.0480); ACC=0.5960 (0.0460)\n",
      "Pipeline: AUC=0.6188 (0.0428); ACC=0.5941 (0.0407)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "linear_pipeline = make_pipeline(clone(feature_selection), clone(linear_model))\n",
    "nonlinear_pipeline = make_pipeline(clone(feature_selection), clone(nonlinear_model))\n",
    "\n",
    "cv_scores_lin = cross_validate(linear_pipeline, X, y, cv=cv, scoring=[\"roc_auc\", \"balanced_accuracy\"], n_jobs=2)\n",
    "cv_scores_nlin = cross_validate(nonlinear_pipeline, X, y, cv=cv, scoring=[\"roc_auc\", \"balanced_accuracy\"], n_jobs=2)\n",
    "\n",
    "print(\"Feature selection + modeling via cross-validation = good:\")\n",
    "report_performance(linear_pipeline, X, y, cv=pd.DataFrame(cv_scores_lin))\n",
    "report_performance(nonlinear_pipeline, X, y, cv=pd.DataFrame(cv_scores_nlin))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Summary*:\n",
    "- If you have data leakage (i.e. using part/all of your test set for preprocessing) you will inflate your performance estimate\n",
    "\n",
    "However, the bias seems to be very small. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 More extreme bias: p >> n case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new classification data set where the number of features is larger than number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p > n regime:\n",
    "n_samples_new, n_features_new = 100, 1_000\n",
    "X_new, y_new = make_classification(n_samples=n_samples_new, n_features=n_features_new, n_classes=n_classes, n_informative=1, n_redundant=0, class_sep=0.2, n_clusters_per_class=1, random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Wrong way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p >> n:\n",
      "Feature selection on all data + cross-validation afterwards = bias:\n",
      "LogisticRegression: AUC=0.9760 (0.0506); ACC=0.9500 (0.0527)\n",
      "RandomForestClassifier: AUC=0.9380 (0.0802); ACC=0.8800 (0.1033)\n"
     ]
    }
   ],
   "source": [
    "feature_selection = SelectPercentile(percentile=10)\n",
    "X_new_sel = feature_selection.fit_transform(X_new, y_new)\n",
    "\n",
    "cv_scores_lin = cross_validate(clone(linear_model), X_new_sel, y_new, cv=cv, scoring=[\"roc_auc\", \"balanced_accuracy\"], n_jobs=2)\n",
    "cv_scores_nlin = cross_validate(clone(nonlinear_model), X_new_sel, y_new, cv=cv, scoring=[\"roc_auc\", \"balanced_accuracy\"], n_jobs=2)\n",
    "\n",
    "print(\"p >> n:\")\n",
    "print(\"Feature selection on all data + cross-validation afterwards = bias:\")\n",
    "report_performance(linear_model, X, y, cv=pd.DataFrame(cv_scores_lin))\n",
    "report_performance(nonlinear_model, X, y, cv=pd.DataFrame(cv_scores_nlin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Right way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p >> n\n",
      "Feature selection + modeling via cross-validation = good:\n",
      "LogisticRegression: AUC=0.5820 (0.1922); ACC=0.5200 (0.1751)\n",
      "RandomForestClassifier: AUC=0.5820 (0.1922); ACC=0.5200 (0.1751)\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_validate(clone(linear_pipeline), X_new, y_new, cv=cv, scoring=[\"roc_auc\", \"balanced_accuracy\"], n_jobs=2)\n",
    "cv_scores = cross_validate(clone(nonlinear_pipeline), X_new, y_new, cv=cv, scoring=[\"roc_auc\", \"balanced_accuracy\"], n_jobs=2)\n",
    "\n",
    "print(\"p >> n\")\n",
    "print(\"Feature selection + modeling via cross-validation = good:\")\n",
    "report_performance(linear_model, X, y, cv=pd.DataFrame(cv_scores))\n",
    "report_performance(nonlinear_model, X, y, cv=pd.DataFrame(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Summary*:\n",
    "- The bias is indeed much larger in the p >> n case (for both linear and non-linear models)\n",
    "- The increased sample size protects against the incorrect validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Experiment: Improper validation while using cross-validation: the case for ~~nasty~~ nested cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Wrong way: Performing a selection of optimal number of features via cross-validation __and__ using the obtained cross-validated values as reported performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFE-CV selection used as reporting:\n",
      "LogisticRegression: AUC=0.6638 (0.0568)\n",
      "RandomForestClassifier: AUC=0.5777 (0.0445)\n"
     ]
    }
   ],
   "source": [
    "rfe_linear = RFECV(estimator=clone(linear_model), cv=cv, scoring=\"roc_auc\", n_jobs=4)\n",
    "rfe_nonlinear = RFECV(estimator=clone(nonlinear_model), cv=cv, scoring=\"roc_auc\", n_jobs=4)\n",
    "\n",
    "rfe_linear.fit(X, y)\n",
    "rfe_nonlinear.fit(X, y)\n",
    "\n",
    "print(\"RFE-CV selection used as reporting:\")\n",
    "print(f\"{linear_model.__class__.__name__}: AUC={rfe_linear.cv_results_['mean_test_score'][0]:.4f} ({rfe_linear.cv_results_['std_test_score'][0]:.4f})\")\n",
    "print(f\"{nonlinear_model.__class__.__name__}: AUC={rfe_nonlinear.cv_results_['mean_test_score'][0]:.4f} ({rfe_nonlinear.cv_results_['std_test_score'][0]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Right (but computational intensive way): nested cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nested-CV](https://vitalflux.com/wp-content/uploads/2020/08/Screenshot-2020-08-30-at-6.33.47-PM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nested cross-validation:\n",
      "LogisticRegression: AUC=0.6638 (0.0598); ACC=0.6201 (0.0463)\n",
      "RandomForestClassifier: AUC=0.6122 (0.0347); ACC=0.5862 (0.0274)\n"
     ]
    }
   ],
   "source": [
    "# inner CV with 5 folds\n",
    "cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "rfe_linear = RFECV(estimator=clone(linear_model), cv=cv_inner, scoring=\"roc_auc\", n_jobs=4)\n",
    "rfe_nonlinear = RFECV(estimator=clone(nonlinear_model), cv=cv_inner, scoring=\"roc_auc\", n_jobs=4)\n",
    "\n",
    "cv_scores_lin = cross_validate(rfe_linear, X, y, cv=cv, scoring=[\"roc_auc\", \"balanced_accuracy\"])\n",
    "cv_scores_nlin = cross_validate(rfe_nonlinear, X, y, cv=cv, scoring=[\"roc_auc\", \"balanced_accuracy\"])\n",
    "\n",
    "print(\"Nested cross-validation:\")\n",
    "report_performance(linear_model, X, y, cv=pd.DataFrame(cv_scores_lin))\n",
    "report_performance(nonlinear_model, X, y, cv=pd.DataFrame(cv_scores_nlin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Summary*:\n",
    "- While nested CV is the correct way of performing feature selection + validation their does not seem to be any bias\n",
    "- Again the large size of the data set *protects* the validation pipeline\n",
    "- The observed difference might come from the fact that I used 10-fold CV for the outer CV and 5-fold CV for the inner CV and therefore the two experiments don't match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Hyperameter tuning + performance reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.1 Wrong way: grid-search via CV on the data set used as reporting metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search via CV and reporting of the same performance:\n",
      "LogisticRegression: Best-AUC: 0.5982\n",
      "RandomForestClassifier: Best-AUC: 0.6212\n"
     ]
    }
   ],
   "source": [
    "# Should use LogisticRegressionCV instead but want to keep it the same\n",
    "linear_model = LogisticRegression(penalty=\"l2\")\n",
    "gs_linear_model = GridSearchCV(estimator=linear_model, param_grid={\"C\": [0.01, 0.1, 1, 10, 100]}, cv=cv, scoring=\"roc_auc\", n_jobs=4)\n",
    "gs_nonlinear_model = GridSearchCV(estimator=clone(nonlinear_model), param_grid={\"max_features\": [\"sqrt\", \"log2\", 0.2, 0.4, 0.6]}, cv=cv, scoring=\"roc_auc\", n_jobs=4)\n",
    "\n",
    "gs_linear_model.fit(X, y)\n",
    "gs_nonlinear_model.fit(X, y)\n",
    "\n",
    "print(\"Grid search via CV and reporting of the same performance:\")\n",
    "print(f\"{linear_model.__class__.__name__}: Best-AUC: {gs_linear_model.best_score_:.4f}\")\n",
    "print(f\"{nonlinear_model.__class__.__name__}: Best-AUC: {gs_nonlinear_model.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.2 Correct way: nested-CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid-search + performance reporting via nested cross-validation:\n",
      "LogisticRegression: AUC=0.5982 (0.0491); ACC=0.5659 (0.0430)\n",
      "RandomForestClassifier: AUC=0.6206 (0.0600); ACC=0.5972 (0.0429)\n"
     ]
    }
   ],
   "source": [
    "gs_linear_model = GridSearchCV(estimator=linear_model, param_grid={\"C\": [0.01, 0.1, 1, 10, 100]}, cv=cv_inner, scoring=\"roc_auc\", n_jobs=4)\n",
    "gs_nonlinear_model = GridSearchCV(estimator=clone(nonlinear_model), param_grid={\"max_features\": [\"sqrt\", \"log2\", 0.2, 0.4, 0.6]}, cv=cv_inner, scoring=\"roc_auc\", n_jobs=4)\n",
    "\n",
    "cv_scores_lin = cross_validate(gs_linear_model, X, y, cv=cv, scoring=[\"roc_auc\", \"balanced_accuracy\"])\n",
    "cv_scores_nlin = cross_validate(gs_nonlinear_model, X, y, cv=cv, scoring=[\"roc_auc\", \"balanced_accuracy\"])\n",
    "\n",
    "print(\"Grid-search + performance reporting via nested cross-validation:\")\n",
    "report_performance(linear_model, X, y, cv=pd.DataFrame(cv_scores_lin))\n",
    "report_performance(nonlinear_model, X, y, cv=pd.DataFrame(cv_scores_nlin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Overall Results (AUC):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                    | Overfitting |    CV    | FS-wrong | FS-right | RFE-CV | RFE-nested-CV | GS-CV | GS-nested-CV |\n",
    "|--------------------|:-----------:|:--------:|:--------:|:--------:|:------:|:-------------:|:-----:|:------------:|\n",
    "|Logistic Regression |   0.7320    |  0.5982  |  0.6798  |  0.6348  | 0.6638 |    0.6638     | 0.5982| 0.5982       |\n",
    "|Random Forest       |   1.0       |  0.6017  |  0.6661  |  0.6188  | 0.5777 |    0.6122     | 0.6212| 0.6206       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Proper validation requires nested CV\n",
    "- Experiments show here that this might not be relevant in the case of n >> p data sets as the large number of samples _protects_ against the potential of overfitting \n",
    "- Bias _might_ be compounded when combining all the steps in a non-nested way\n",
    "- Bias is always huge if p >> n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47de4e394668de6e6b7bb14e19cebd8fed5df330d3702969680dc5ab9e8dabbf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
